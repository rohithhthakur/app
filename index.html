import pandas as pd

# Paths to the Parquet files on S3
file_path_1 = 's3://your-bucket-name/path/to/your-first-file.parquet'
file_path_2 = 's3://your-bucket-name/path/to/your-second-file.parquet'

# Read the Parquet files into pandas DataFrames
df1 = pd.read_parquet(file_path_1)
df2 = pd.read_parquet(file_path_2)

# Check if columns are the same
columns_same = set(df1.columns) == set(df2.columns)
print("Are columns the same in both files?", columns_same)

# Function to compare schemas and create a DataFrame for display
def compare_schemas(df1, df2):
    schema1 = {col: str(dtype) for col, dtype in zip(df1.columns, df1.dtypes)}
    schema2 = {col: str(dtype) for col, dtype in zip(df2.columns, df2.dtypes)}

    # Creating lists to hold comparison data
    columns = list(set(schema1.keys()).union(set(schema2.keys())))
    exists_in_file1 = []
    type_in_file1 = []
    exists_in_file2 = []
    type_in_file2 = []
    
    # Populate the lists with schema information
    for col in columns:
        exists_in_file1.append(col in schema1)
        type_in_file1.append(schema1.get(col, 'N/A'))
        exists_in_file2.append(col in schema2)
        type_in_file2.append(schema2.get(col, 'N/A'))
    
    # Create a DataFrame from the comparison data
    comparison_df = pd.DataFrame({
        'Column': columns,
        'Exists in File 1': exists_in_file1,
        'Type in File 1': type_in_file1,
        'Exists in File 2': exists_in_file2,
        'Type in File 2': type_in_file2
    })
    
    return comparison_df

# Get the schema comparison DataFrame
schema_comparison_df = compare_schemas(df1, df2)

# Display the schema comparison DataFrame
print(schema_comparison_df)