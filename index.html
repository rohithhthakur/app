import pandas as pd

# Paths to the Parquet files on S3
file_path_1 = 's3://your-bucket-name/path/to/your-first-file.parquet'
file_path_2 = 's3://your-bucket-name/path/to/your-second-file.parquet'

# Read the Parquet files into pandas DataFrames
df1 = pd.read_parquet(file_path_1)
df2 = pd.read_parquet(file_path_2)

# Extracting schemas
schema1 = {col: str(dtype) for col, dtype in zip(df1.columns, df1.dtypes)}
schema2 = {col: str(dtype) for col, dtype in zip(df2.columns, df2.dtypes)}

# Identify missing and extra columns
missing_columns = set(schema2.keys()) - set(schema1.keys())
extra_columns = set(schema1.keys()) - set(schema2.keys())

# Find columns with type mismatches
type_mismatches = {col: (schema1[col], schema2[col]) for col in schema1 if col in schema2 and schema1[col] != schema2[col]}

# Output results
if missing_columns:
    print("Columns missing in File 1 that are present in File 2 (Source of Truth):")
    print(missing_columns)

if extra_columns:
    print("Extra columns in File 1 not present in File 2 (Source of Truth):")
    print(extra_columns)

if type_mismatches:
    print("Columns with type mismatches:")
    for col, types in type_mismatches.items():
        print(f"Column: {col}, Type in File 1: {types[0]}, Type in File 2: {types[1]}")
else:
    print("No type mismatches found.")