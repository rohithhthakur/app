# Step 1: Read the Parquet files from S3
file1_path = "s3://your-bucket/path/to/file1.parquet"
file2_path = "s3://your-bucket/path/to/file2.parquet"

df1 = spark.read.parquet(file1_path)
df2 = spark.read.parquet(file2_path)

# Step 2: Print the columns of each DataFrame
print("Columns in file 1:")
print(df1.columns)

print("Columns in file 2:")
print(df2.columns)

# Step 3: Identify common columns
common_columns = list(set(df1.columns).intersection(set(df2.columns)))
print("Common columns:")
print(common_columns)

# Step 4: Compare the schemas of the common columns
schema1 = df1.schema
schema2 = df2.schema

differences = []

for col in common_columns:
    type1 = schema1[col].dataType
    type2 = schema2[col].dataType
    if type1 != type2:
        differences.append((col, type1, type2))

if differences:
    print("Differences in common column schemas:")
    for diff in differences:
        print(f"Column: {diff[0]} - File 1 Type: {diff[1]}, File 2 Type: {diff[2]}")
else:
    print("No differences found in common column schemas.")